/*
 * Copyright 2025 The llm-d Authors.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <torch/extension.h>
#include <ATen/cuda/CUDAContext.h>
#include <c10/cuda/CUDAGuard.h>
#include <cuda_runtime.h>
#include <iostream>
#include <thread>
#include <mutex>
#include <queue>
#include <condition_variable>
#include <atomic>
#include <sys/syscall.h>
#include <unistd.h>
#include <numa.h>

#include "thread_pool.hpp"
#include "numa_utils.hpp"
#include "debug_utils.hpp"

// Minimum staging buffer size: 16 MB
const size_t MIN_STAGING_BUFFER_SIZE = 16 * 1024 * 1024;

// Thread-local index for CUDA streams
extern thread_local size_t thread_stream_idx;
// Thread-local buffer used by each IO thread
thread_local StagingBufferInfo t_staging_buffer{};

// ThreadPool constructor
ThreadPool::ThreadPool(size_t threads,
                       size_t staging_buffer_mb,
                       int tp_rank,
                       int device_id)
    : m_device_id(device_id) {
    // Get GPU NUMA node ONCE outside the thread loop
    std::vector<int> local_cpus;
    int gpu_numa = get_gpu_numa_node(device_id);
    if (gpu_numa >= 0) {
        std::cout << "[INFO] GPU " << device_id << " mapped to NUMA node "
                  << gpu_numa << "\n";

        // Get all CPUs in that NUMA node
        local_cpus = get_cpus_in_numa_node(gpu_numa);
    }

    if (local_cpus.empty()) {
        std::cerr << "[WARN] No CPUs found for NUMA node " << gpu_numa
                  << ". System may not be NUMA-aware. Using all CPUs.\n";
        // Populate with all available CPUs as fallback
        int num_cpus = sysconf(_SC_NPROCESSORS_ONLN);
        for (int i = 0; i < num_cpus; ++i) {
            local_cpus.push_back(i);
        }
    }

    // Log available CPUs
    std::cout << "[INFO] CPUs available for GPU " << device_id << " (NUMA "
              << gpu_numa << "): ";
    for (int cpu : local_cpus) std::cout << cpu << " ";
    std::cout << "\n";

    // Create all worker threads
    for (size_t i = 0; i < threads; ++i) {
        // Launch a new worker thread with a lambda that initializes thread
        // resources and processes queued tasks.
        workers.emplace_back([this,
                              i,
                              threads,
                              staging_buffer_mb,
                              tp_rank,
                              device_id,
                              gpu_numa,
                              local_cpus] {
            cudaSetDevice(device_id);

            // Round-robin CPUs within the NUMA node
            // TODO: Re-evaluate whether strict NUMA-based round-robin CPU
            // assignment is optimal for performance.
            int cpu_id = local_cpus[i % local_cpus.size()];

            cpu_set_t cpuset;
            CPU_ZERO(&cpuset);
            CPU_SET(cpu_id, &cpuset);

            if (pthread_setaffinity_np(pthread_self(),
                                       sizeof(cpuset),
                                       &cpuset) != 0) {
                std::cerr << "[ERROR] Failed to set affinity for thread " << i
                          << " to CPU " << cpu_id << "\n";
            }

            DEBUG_PRINT("IO thread " << i << " set CUDA device to " << device_id
                                     << ", tp_rank=" << tp_rank
                                     << ") pinned to CPU " << cpu_id);

            // Allocate thread-local staging buffer for this IO thread
            size_t alloc_bytes = staging_buffer_mb * 1024 * 1024;
            StagingBufferInfo buf =
                get_thread_local_staging_buffer(alloc_bytes);

            if (!buf.ptr) {
                std::cerr << "[ERROR] Failed to allocate staging buffer for IO "
                             "thread "
                          << i << "\n";
                return;
            }

            t_staging_buffer.ptr = buf.ptr;
            t_staging_buffer.size = buf.size;

            DEBUG_PRINT("IO thread " << i << " allocated staging buffer "
                                     << (t_staging_buffer.size / (1024 * 1024))
                                     << " MB");

            // Each thread gets its own CUDA stream index
            thread_stream_idx = i;

            // Worker loop
            while (true) {
                std::function<void()> task;
                {
                    // Lock the task queue before checking it
                    std::unique_lock<std::mutex> lock(queue_mutex);

                    // Wait until either a new task arrives or the pool is
                    // stopping. (wait() unlocks the mutex while sleeping and
                    // re-locks it when waking)
                    condition.wait(lock,
                                   [this] { return stop || !tasks.empty(); });

                    // Exit thread if pool is stopping and no tasks remain
                    if (stop && tasks.empty()) return;

                    // Fetch next task from the queue
                    task = std::move(tasks.front());
                    tasks.pop();
                }
                try {
                    // Execute the task
                    task();
                } catch (const std::exception& e) {
                    std::cerr
                        << "[ERROR] Exception in worker thread: " << e.what()
                        << "\n";
                } catch (...) {
                    std::cerr << "[ERROR] Unknown exception in worker thread\n";
                }
            }
        });
    }

    std::cout << "[INFO] All " << threads
              << " I/O threads initialized with staging buffers\n";
}

// ThreadPool destructor
ThreadPool::~ThreadPool() {
    stop = true;
    condition.notify_all();
    // Wait for all worker threads to exit
    for (std::thread& worker : workers) {
        worker.join();
    }
}

// Return thread-local staging buffer, allocating or reallocating if needed
StagingBufferInfo get_thread_local_staging_buffer(size_t required_bytes) {
    if (!t_staging_buffer.ptr || t_staging_buffer.size < required_bytes) {
        if (t_staging_buffer.ptr) {
            cudaFreeHost(t_staging_buffer.ptr);
            t_staging_buffer.ptr = nullptr;
            t_staging_buffer.size = 0;
            std::cerr << "[WARN] Thread " << std::this_thread::get_id()
                      << " existing staging buffer too small ("
                      << t_staging_buffer.size << " bytes), reallocating "
                      << required_bytes << " bytes\n";
        }

        size_t alloc_size = std::max(required_bytes, MIN_STAGING_BUFFER_SIZE);
        cudaError_t err =
            cudaHostAlloc(&t_staging_buffer.ptr,
                          alloc_size,
                          cudaHostAllocMapped | cudaHostAllocPortable);

        if (err != cudaSuccess) {
            std::cerr << "[ERROR] cudaHostAlloc failed: "
                      << cudaGetErrorString(err) << "\n";
            t_staging_buffer.ptr = nullptr;
            t_staging_buffer.size = 0;
        } else {
            t_staging_buffer.size = alloc_size;
            DEBUG_PRINT("[INFO] Thread " << std::this_thread::get_id()
                                         << " allocated staging buffer "
                                         << (alloc_size / (1024 * 1024))
                                         << " MB");
        }
    }

    return t_staging_buffer;
}
